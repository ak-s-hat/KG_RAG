[
    {
        "input": "What limitations do ARIMA models face in power load forecasting regarding non-linear sequences, parameter adjustment, and multivariate processing?",
        "actual_output": null,
        "expected_output": "ARIMA models face three main limitations in power load forecasting: (1) They struggle with non-linear sequences, as they are designed for linear relationships and cannot effectively capture complex non-linear patterns. (2) Parameter adjustment is challenging due to the need for manual tuning of (p, d, q) parameters, which complicates model optimization. (3) They cannot process multivariate time series, as they are inherently univariate and lack the capability to incorporate external variables like weather or economic factors.",
        "context": [
            "Electronics 2023, 12, 2175 2 of 19\nto realize the dynamic balance between power generation and charge change, researchers\nmostly study the power load based on the time series [3] for power load forecasting. Until\nnow, there have been traditional models [ 4] and arti\ufb01cial intelligence (AI) models [ 5]\nfor prediction.\nTraditional modeling is based on statistical analysis and has good interpretability. The\nautoregressive moving average model (ARMA) is a classical statistical modeling method [6].\nNowicka-Zagrajek et al. [7] applied the ARMA model to California\u2019s short-term power\nforecasting and achieved good results. However, ARMA is only suitable for stationary\nstochastic processes [ 7]. Most sequences in nature are non-stationary. Therefore, the\nresearchers proposed an autoregressive integrated moving average (ARIMA) model [8],\nwhich converts non-stationary sequences into stationary sequences based on differential\noperations. Valipour et al. [ 9] used the ARIMA model to predict the monthly in\ufb02ow of\nDeziba Reservoir, and the prediction results are better than ARMA model. In addition,\ntraditional modeling can achieve good results in solving linear problems [10], but it cannot\nsolve nonlinear problems well and cannot deal with multivariate time series problems.\nAI modeling is data-driven, which has been widely used in power load forecasting,\nsuch as back propagation (BP) [11] and arti\ufb01cial neural network (ANN) [12]. In general,\npower load forecasting needs to be abstracted into time series forecasting, while traditional\nBP neural networks cannot deal with time series problems well. A.S. Carpinteiro et al. [13]\nproposed the use of ANN models to make long-term predictions of future power loads\nusing data obtained from North American Electric Power Corporation. The recurrent neural\nnetwork (RNN) is a special kind of ANN [14], which retains a small amount of previous\ninformation through a self-connected structure, so as to establish the relationship between\nthe past and the present. Tomonobu et al. [15] applied RNNs to long-term forecasting of\nwind power generation. This model is more accurate than the feed-forward neural",
            "Electronics 2023, 12, 2175 3 of 19\nterm power forecasting (LTLF) [ 23]. LTLF allows people to \ufb01nd and evaluate suitable\nphotovoltaic power generation locations on a large scale. In the above papers, the authors\nfound that the ensemble learning model has better performance after comparing the effects\nof the ensemble model and the single model. In addition, economic, environmental, and\nother factors also affect the consumption of power load [ 24]. Fan et al. enhanced the\nprecision of power load by adding weather multivariate variables [25]. It can be seen that\nmultivariate prediction has improved power prediction.\n1.2. Reasearch Gap\nThrough the background investigation of Section 1.1, the advantages of the traditional\nmodel are simple and interpretable. However, the performance on nonlinear and non-\nstationary sequences is not ideal. Even if the ARIMA model enhances the ability to solve\nnon-stationary sequences, it still does not have the ability to solve nonlinear sequences\nwell. Although the ARIMA model can solve the non-stationary problem, its parameters are\ndif\ufb01cult to adjust. At the same time, traditional algorithms cannot handle multivariate time\nseries problems. In AI modeling, the BP neural network [26] model cannot capture time\ndomain information. The RNN [27] model can capture time-domain information, but due\nto the problem of gradient vanishing and gradient exploding, the RNN model does not\nperform well in long-term prediction. The LSTM model alleviates the problem of gradient\nvanishing and gradient exploding to a certain extent by adding gating units, and has the\nability of long-term prediction, but there are also large errors. The Attention mechanism\nsolves the long-term dependence of input and output by calculating the correlation of all\ndata and has good long-term prediction ability. However, it is not sensitive to short-term\nsequence features, resulting in large errors in the \ufb01nal results.\nIn summary, a single model always has its limitations, and the ensemble models\ncan compensate for the shortcomings of a single model by integrating the advantages\nof multiple individuals. In previous studies, most of the work focused on the use of a\nsingle model for tuning and failed to combine the advantages of each model to improve\nperformance.",
            " Engineering and Automated Learning, Madrid, Spain, 21\u201323 November 2018;\nSpringer: Cham, Switzerland, 2018; pp. 481\u2013490.\n4. Fan, D.; Sun, H.; Yao, J.; Zhang, K.; Yan, X.; Sun, Z. Well production forecasting based on ARIMA-LSTM model considering\nmanual operations. Energy 2021, 220, 119708. [CrossRef]\n5. Ahmad, T.; Zhang, D.; Huang, C.; Zhang, H.; Dai, N.; Song, Y.; Chen, H. Arti\ufb01cial intelligence in sustainable energy industry:\nStatus Quo, challenges and opportunities. J. Clean. Prod. 2021, 289, 125834. [CrossRef]\n6. Yu, C.; Li, Y.; Chen, Q.; Lai, X.; Zhao, L. Matrix-based wavelet transformation embedded in recurrent neural networks for wind\nspeed prediction. Appl. Energy 2022, 324, 119692. [CrossRef]\n7. Nowicka-Zagrajek, J.; Weron, R. Modeling electricity loads in California: ARMA models with hyperbolic noise. Signal Process.\n2002, 82, 1903\u20131915. [CrossRef]\n8. Chen, X.; Jia, S.; Ding, L.; Xiang, Y. Reasoning over temporal knowledge graph with temporal consistency constraints. J. Intell.\nFuzzy Syst. 2021, 40, 11941\u201311950. [CrossRef]\n9. Valipour, M.; Banihabib, M.E.; Behbahani, S.M.R. Comparison of the ARMA, ARIMA, and the autoregressive arti\ufb01cial neural\nnetwork models in forecasting the monthly in\ufb02ow of Dez dam reservoir. J. Hydrol. 2013, 476, 433\u2013441. [CrossRef]\n10. Divina, F.; Gilson, A.; Gom \u00e9z-Vela, F.; Garc\u00eda Torres, M.; Torres, J.F. Stacking ensemble learning for short-term electricity\nconsumption forecasting. Energies 2018, 11, 949. [CrossRef]\n11. Gu, B.; Shen, H.; Lei, X.; Hu, H.; Liu, X. Forecasting and",
            "STM-Informer model can not only capture short-term time\ncorrelation but can also accurately predict long-term power load. In this paper, a one-year dataset of\nthe distribution network in the city of Tetouan in northern Morocco was used for experiments, and\nthe mean square error (MSE) and mean absolute error (MAE) were used as evaluation criteria. The\nlong-term prediction of this model is 0.58 and 0.38 higher than that of the lstm model based on MSE\nand MAE. The experimental results show that the LSTM-Informer model based on ensemble learning\nhas more advantages in long-term power load forecasting than the advanced baseline method.\nKeywords: ensemble learning; energy consumption forecasting; neural networks; long-term load\nforecasting\n1. Introduction\n1.1. Background and Literature Review\nIn 2021, the share of electricity in global \ufb01nal consumption increased by 0.2 points,\nreaching 20.4% [1]. Electricity consumption has been increasing in recent years. It can\nbe seen that electricity is becoming more and more important in our daily life. With the\nincreasing demand for electricity, the country needs to build more power stations to meet\nthe needs of human production. The purpose of the establishment of the national power\nsystem is to meet the power demand [ 2]. If there is no accurate prediction of long-term\npower load, it will lead to too many power generation facilities or insuf\ufb01cient power\ngeneration facilities. Excessive establishment of power generation facilities will lead to a\nwaste of electricity and affect economic decision-making. The lack of power generation\nfacilities is more serious, which may lead to insuf\ufb01cient power supply and affect people\u2019s\ndaily life. Nowadays, the main task of power companies is to predict the power load so\nas to adjust the power supply and study the expansion planning of power generation\nfacilities. This paper studies the long-term power forecasting, aiming to solve the problem\nof expansion planning and transformation of the power system.\nBecause the electric energy in the power grid system cannot easily be stored in large\nquantities and the power demand changes all the time, the power company needs the\nsystem to generate electricity and charge changes to achieve dynamic balance. In order\nElectronics 2023, 12, 2175.",
            "\u00a0event\u00a0correlation,\u00a0the\u00a0Informer\u00a0model\u00a0successfully\u00a0\nsolves\u00a0the\u00a0long\u2010term\u00a0dependence\u00a0problem.\u00a0Therefore,\u00a0in\u00a0long\u2010term\u00a0power\u00a0load\u00a0forecast\u2010\ning,\u00a0the\u00a0LSTM\u2010Informer\u00a0model\u00a0can\u00a0still\u00a0achieve\u00a0the\u00a0greatest\u00a0results.\u00a0\nFigure 11. Comparison results of six long-term power load models via MSE and MAE metric.\nCombining the results of the two tables, we can conclude that the LSTM-Informer\nmodel has improved the performance of the base learner on the short-term dependence\nproblem. It has relatively good performance compared with other more advanced models\nwith similar architectures. On the issue of long-term dependence, in most cases, it is\nsuperior to other single models. Although the Transformer model performs well in long-\nterm prediction, its accuracy in short-term prediction is less than 50% of the LSTM-Informer\nperformance. If a model is needed for power load forecasting, the LSTM-Informer model\nhas the best performance. It is optimal in both STLF and LTLF.\n4.6.2. Results Analysis\nIn order to further compare the differences between the LSTM-Informer model, In-\nformer model, Autoformer model, Transformer model, Reformer model, and LSTM model,\nwe will analyze the results in detail."
        ],
        "retrieval_context": null,
        "additional_metadata": {
            "evolutions": [
                "Concretizing"
            ],
            "synthetic_input_quality": 1.0
        },
        "comments": null,
        "tools_called": null,
        "expected_tools": null,
        "source_file": "D:\\ML\\my_rag\\electronics-12-02175.pdf",
        "name": null,
        "custom_column_key_values": null,
        "images_mapping": null
    },
    {
        "input": "Analyze how integrating LSTM and Informer's strengths in short-term and long-term dependencies improves forecasting accuracy compared to standalone models in power load prediction.",
        "actual_output": null,
        "expected_output": "Integrating LSTM and Informer models enhances power load forecasting by combining LSTM's short-term dependency capture with Informer's long-term correlation handling. The LSTM-Informer ensemble addresses both temporal scales, outperforming standalone models like LSTM, Informer, or Transformer in metrics (MSE/MAE). This hybrid approach mitigates LSTM's long-term prediction errors and Informer's short-term limitations, achieving superior accuracy in both short-term load forecasting (STLF) and long-term load forecasting (LTLF) compared to traditional models (ARIMA, RNN) and advanced baselines.",
        "context": [
            "Electronics 2023, 12, 2175 2 of 19\nto realize the dynamic balance between power generation and charge change, researchers\nmostly study the power load based on the time series [3] for power load forecasting. Until\nnow, there have been traditional models [ 4] and arti\ufb01cial intelligence (AI) models [ 5]\nfor prediction.\nTraditional modeling is based on statistical analysis and has good interpretability. The\nautoregressive moving average model (ARMA) is a classical statistical modeling method [6].\nNowicka-Zagrajek et al. [7] applied the ARMA model to California\u2019s short-term power\nforecasting and achieved good results. However, ARMA is only suitable for stationary\nstochastic processes [ 7]. Most sequences in nature are non-stationary. Therefore, the\nresearchers proposed an autoregressive integrated moving average (ARIMA) model [8],\nwhich converts non-stationary sequences into stationary sequences based on differential\noperations. Valipour et al. [ 9] used the ARIMA model to predict the monthly in\ufb02ow of\nDeziba Reservoir, and the prediction results are better than ARMA model. In addition,\ntraditional modeling can achieve good results in solving linear problems [10], but it cannot\nsolve nonlinear problems well and cannot deal with multivariate time series problems.\nAI modeling is data-driven, which has been widely used in power load forecasting,\nsuch as back propagation (BP) [11] and arti\ufb01cial neural network (ANN) [12]. In general,\npower load forecasting needs to be abstracted into time series forecasting, while traditional\nBP neural networks cannot deal with time series problems well. A.S. Carpinteiro et al. [13]\nproposed the use of ANN models to make long-term predictions of future power loads\nusing data obtained from North American Electric Power Corporation. The recurrent neural\nnetwork (RNN) is a special kind of ANN [14], which retains a small amount of previous\ninformation through a self-connected structure, so as to establish the relationship between\nthe past and the present. Tomonobu et al. [15] applied RNNs to long-term forecasting of\nwind power generation. This model is more accurate than the feed-forward neural",
            "Electronics 2023, 12, 2175 3 of 19\nterm power forecasting (LTLF) [ 23]. LTLF allows people to \ufb01nd and evaluate suitable\nphotovoltaic power generation locations on a large scale. In the above papers, the authors\nfound that the ensemble learning model has better performance after comparing the effects\nof the ensemble model and the single model. In addition, economic, environmental, and\nother factors also affect the consumption of power load [ 24]. Fan et al. enhanced the\nprecision of power load by adding weather multivariate variables [25]. It can be seen that\nmultivariate prediction has improved power prediction.\n1.2. Reasearch Gap\nThrough the background investigation of Section 1.1, the advantages of the traditional\nmodel are simple and interpretable. However, the performance on nonlinear and non-\nstationary sequences is not ideal. Even if the ARIMA model enhances the ability to solve\nnon-stationary sequences, it still does not have the ability to solve nonlinear sequences\nwell. Although the ARIMA model can solve the non-stationary problem, its parameters are\ndif\ufb01cult to adjust. At the same time, traditional algorithms cannot handle multivariate time\nseries problems. In AI modeling, the BP neural network [26] model cannot capture time\ndomain information. The RNN [27] model can capture time-domain information, but due\nto the problem of gradient vanishing and gradient exploding, the RNN model does not\nperform well in long-term prediction. The LSTM model alleviates the problem of gradient\nvanishing and gradient exploding to a certain extent by adding gating units, and has the\nability of long-term prediction, but there are also large errors. The Attention mechanism\nsolves the long-term dependence of input and output by calculating the correlation of all\ndata and has good long-term prediction ability. However, it is not sensitive to short-term\nsequence features, resulting in large errors in the \ufb01nal results.\nIn summary, a single model always has its limitations, and the ensemble models\ncan compensate for the shortcomings of a single model by integrating the advantages\nof multiple individuals. In previous studies, most of the work focused on the use of a\nsingle model for tuning and failed to combine the advantages of each model to improve\nperformance.",
            " Engineering and Automated Learning, Madrid, Spain, 21\u201323 November 2018;\nSpringer: Cham, Switzerland, 2018; pp. 481\u2013490.\n4. Fan, D.; Sun, H.; Yao, J.; Zhang, K.; Yan, X.; Sun, Z. Well production forecasting based on ARIMA-LSTM model considering\nmanual operations. Energy 2021, 220, 119708. [CrossRef]\n5. Ahmad, T.; Zhang, D.; Huang, C.; Zhang, H.; Dai, N.; Song, Y.; Chen, H. Arti\ufb01cial intelligence in sustainable energy industry:\nStatus Quo, challenges and opportunities. J. Clean. Prod. 2021, 289, 125834. [CrossRef]\n6. Yu, C.; Li, Y.; Chen, Q.; Lai, X.; Zhao, L. Matrix-based wavelet transformation embedded in recurrent neural networks for wind\nspeed prediction. Appl. Energy 2022, 324, 119692. [CrossRef]\n7. Nowicka-Zagrajek, J.; Weron, R. Modeling electricity loads in California: ARMA models with hyperbolic noise. Signal Process.\n2002, 82, 1903\u20131915. [CrossRef]\n8. Chen, X.; Jia, S.; Ding, L.; Xiang, Y. Reasoning over temporal knowledge graph with temporal consistency constraints. J. Intell.\nFuzzy Syst. 2021, 40, 11941\u201311950. [CrossRef]\n9. Valipour, M.; Banihabib, M.E.; Behbahani, S.M.R. Comparison of the ARMA, ARIMA, and the autoregressive arti\ufb01cial neural\nnetwork models in forecasting the monthly in\ufb02ow of Dez dam reservoir. J. Hydrol. 2013, 476, 433\u2013441. [CrossRef]\n10. Divina, F.; Gilson, A.; Gom \u00e9z-Vela, F.; Garc\u00eda Torres, M.; Torres, J.F. Stacking ensemble learning for short-term electricity\nconsumption forecasting. Energies 2018, 11, 949. [CrossRef]\n11. Gu, B.; Shen, H.; Lei, X.; Hu, H.; Liu, X. Forecasting and",
            "STM-Informer model can not only capture short-term time\ncorrelation but can also accurately predict long-term power load. In this paper, a one-year dataset of\nthe distribution network in the city of Tetouan in northern Morocco was used for experiments, and\nthe mean square error (MSE) and mean absolute error (MAE) were used as evaluation criteria. The\nlong-term prediction of this model is 0.58 and 0.38 higher than that of the lstm model based on MSE\nand MAE. The experimental results show that the LSTM-Informer model based on ensemble learning\nhas more advantages in long-term power load forecasting than the advanced baseline method.\nKeywords: ensemble learning; energy consumption forecasting; neural networks; long-term load\nforecasting\n1. Introduction\n1.1. Background and Literature Review\nIn 2021, the share of electricity in global \ufb01nal consumption increased by 0.2 points,\nreaching 20.4% [1]. Electricity consumption has been increasing in recent years. It can\nbe seen that electricity is becoming more and more important in our daily life. With the\nincreasing demand for electricity, the country needs to build more power stations to meet\nthe needs of human production. The purpose of the establishment of the national power\nsystem is to meet the power demand [ 2]. If there is no accurate prediction of long-term\npower load, it will lead to too many power generation facilities or insuf\ufb01cient power\ngeneration facilities. Excessive establishment of power generation facilities will lead to a\nwaste of electricity and affect economic decision-making. The lack of power generation\nfacilities is more serious, which may lead to insuf\ufb01cient power supply and affect people\u2019s\ndaily life. Nowadays, the main task of power companies is to predict the power load so\nas to adjust the power supply and study the expansion planning of power generation\nfacilities. This paper studies the long-term power forecasting, aiming to solve the problem\nof expansion planning and transformation of the power system.\nBecause the electric energy in the power grid system cannot easily be stored in large\nquantities and the power demand changes all the time, the power company needs the\nsystem to generate electricity and charge changes to achieve dynamic balance. In order\nElectronics 2023, 12, 2175.",
            "\u00a0event\u00a0correlation,\u00a0the\u00a0Informer\u00a0model\u00a0successfully\u00a0\nsolves\u00a0the\u00a0long\u2010term\u00a0dependence\u00a0problem.\u00a0Therefore,\u00a0in\u00a0long\u2010term\u00a0power\u00a0load\u00a0forecast\u2010\ning,\u00a0the\u00a0LSTM\u2010Informer\u00a0model\u00a0can\u00a0still\u00a0achieve\u00a0the\u00a0greatest\u00a0results.\u00a0\nFigure 11. Comparison results of six long-term power load models via MSE and MAE metric.\nCombining the results of the two tables, we can conclude that the LSTM-Informer\nmodel has improved the performance of the base learner on the short-term dependence\nproblem. It has relatively good performance compared with other more advanced models\nwith similar architectures. On the issue of long-term dependence, in most cases, it is\nsuperior to other single models. Although the Transformer model performs well in long-\nterm prediction, its accuracy in short-term prediction is less than 50% of the LSTM-Informer\nperformance. If a model is needed for power load forecasting, the LSTM-Informer model\nhas the best performance. It is optimal in both STLF and LTLF.\n4.6.2. Results Analysis\nIn order to further compare the differences between the LSTM-Informer model, In-\nformer model, Autoformer model, Transformer model, Reformer model, and LSTM model,\nwe will analyze the results in detail."
        ],
        "retrieval_context": null,
        "additional_metadata": {
            "evolutions": [
                "Reasoning"
            ],
            "synthetic_input_quality": 1.0
        },
        "comments": null,
        "tools_called": null,
        "expected_tools": null,
        "source_file": "D:\\ML\\my_rag\\electronics-12-02175.pdf",
        "name": null,
        "custom_column_key_values": null,
        "images_mapping": null
    },
    {
        "input": "Assess LSTM-Informer's gradient vanishing mitigation in long-term power load forecasting vs. Transformer models.",
        "actual_output": null,
        "expected_output": "The LSTM-Informer model mitigates gradient vanishing in long-term power load forecasting by combining LSTM's hidden state memory with the Transformer's self-attention mechanism. While standard LSTMs suffer accuracy degradation over longer prediction horizons (as seen in Figure 13), the LSTM-Informer maintains stable performance. Transformer models inherently avoid gradient vanishing through self-attention and position encoding, but the hybrid architecture of LSTM-Informer leverages both short-term correlation capture (via LSTM) and long-term dependency handling (via Informer's Transformer structure), outperforming standalone LSTMs and Transformers in long-term forecasting.",
        "context": [
            "Electronics 2023, 12, 2175 14 of 19\nThe experimental results of MAE of our selected model are shown inFigures 12 and 13.\nFigure 12 shows the MAE index of the selected model using 48 h of data to predict the\npower load of 4 h, 8 h, 12 h, 16 h, and 20 h, respectively. We can see that in the issues of STLF,\nthe LSTM model is superior to most of the Transformer structure models, indicating that\npreserving past information through hidden states is more important for STLF. The LSTM-\nInformer model uses the LSTM model at the bottom to better \ufb01nd the time correlation\nbetween data, so that it pays attention to the time correlation of short-term data when\ntraining in the Informer model. Therefore, in short-term power load forecasting, the\nLSTM-Informer model has achieved good results. Figure 13 shows the MAE index of the\nselected model using 8 h of data to predict the power load of 24 h, 32 h, 40 h, 48 h and\n72 h respectively. In the comparison models, the Informer model, Autoformer model, and\nReformer model are all improved models based on the Transformer model. The transformer\nmodel is mainly composed of an encoder and decoder. Its internal self-attention mechanism\nenables data at each time point to pay attention to the data at other time points. In solving\nthe time series problem, it embeds the time-tamp into the data through position encoding.\nThe self-attention mechanism and position coding make it independent of the past hidden\nstate to capture the dependence on the previous data, so it does not produce the gradient\nvanishing problem. At this time, the improved model based on Transformer has achieved\ngood results, and as the time of power load to be predicted increases, the performance is\nalso in a good state. However, as the time of the power load to be predicted increases, the\naccuracy of the LSTM model gradually decreases, indicating that the LSTM model has a\ngradient vanishing problem. However, the accuracy of the LSTM-Informer model proposed\ndoes not show a signi\ufb01cant trend that the performance gradually deteriorates with the\nincrease of the prediction length, indicating that when the LST",
            "\u00a0LTLF.\u00a0\n4.6.2.\u00a0Results\u00a0Analysis\u00a0\nIn\u00a0order\u00a0to\u00a0further\u00a0compare\u00a0the\u00a0differences\u00a0between\u00a0the\u00a0LSTM\u2010Informer\u00a0model,\u00a0In\u2010\nformer\u00a0 model,\u00a0 Autoformer\u00a0 model,\u00a0 Transformer\u00a0 model,\u00a0 Reformer\u00a0 model,\u00a0 and\u00a0 LSTM\u00a0\nmodel,\u00a0we\u00a0will\u00a0analyze\u00a0the\u00a0results\u00a0in\u00a0detail.\u00a0\nThe\u00a0experimental\u00a0results\u00a0of\u00a0MAE\u00a0of\u00a0our\u00a0selected\u00a0model\u00a0are\u00a0shown\u00a0in\u00a0Figures\u00a012\u00a0and\u00a0\n13.\u00a0Figure\u00a012\u00a0shows\u00a0the\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0selected\u00a0model\u00a0using\u00a048\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0\npower\u00a0load\u00a0of\u00a04\u00a0h,\u00a08\u00a0h,\u00a012\u00a0h,\u00a016\u00a0h,\u00a0and\u00a020\u00a0h,\u00a0respectively.\u00a0We\u00a0can\u00a0see\u00a0that\u00a0in\u00a0the\u00a0issues\u00a0of\u00a0\nSTLF,\u00a0the\u00a0LSTM\u00a0model\u00a0is\u00a0superior\u00a0to\u00a0most\u00a0of\u00a0the\u00a0Transformer\u00a0structure\u00a0models,\u00a0indicating\u00a0\nthat\u00a0preserving\u00a0past\u00a0information\u00a0through\u00a0hidden\u00a0states\u00a0is\u00a0more\u00a0important\u00a0for\u00a0STLF.\u00a0The\u00a0\nLSTM\u2010Informer\u00a0model\u00a0uses\u00a0the\u00a0LSTM\u00a0model\u00a0at\u00a0the\u00a0bottom\u00a0to\u00a0better\u00a0find\u00a0the\u00a0time\u00a0correla\u2010\ntion\u00a0between\u00a0data,\u00a0so\u00a0that\u00a0it\u00a0pays\u00a0attention\u00a0to\u00a0the\u00a0time\u00a0correlation\u00a0of\u00a0short\u2010term\u00a0data\u00a0when\u00a0\ntraining\u00a0 in\u00a0 the\u00a0 Informer\u00a0 model.\u00a0 Therefore,\u00a0 in\u00a0 short\u2010term\u00a0 power\u00a0 load\u00a0 forecasting,\u00a0 the\u00a0\nLSTM\u2010Informer\u00a0model\u00a0has\u00a0achieved\u00a0good\u00a0results.\u00a0Figure\u00a013\u00a0shows\u00a0the\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0\nselected\u00a0model\u00a0using\u00a08\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0power\u00a0load\u00a0of\u00a024\u00a0h,\u00a032\u00a0h,\u00a040\u00a0h,\u00a048\u00a0h\u00a0and\u00a072\u00a0",
            "M model is capturing short-\nterm event correlation, the Informer model successfully solves the long-term dependence\nproblem. Therefore, in long-term power load forecasting, the LSTM-Informer model can\nstill achieve the greatest results.\nElectronics\u00a02023,\u00a012,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 15\u00a0 of\u00a0 19\u00a0\n\u00a0\n\u00a0\n\u00a0\nFigure\u00a012.\u00a0The\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0selected\u00a0model\u00a0using\u00a048\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0power\u00a0load\u00a0of\u00a04\u00a0h,\u00a0\n8\u00a0h,\u00a012\u00a0h,\u00a016\u00a0h,\u00a0and\u00a020\u00a0h.\u00a0\n\u00a0\nFigure\u00a013.\u00a0The\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0selected\u00a0model\u00a0uses\u00a08\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0power\u00a0load\u00a0of\u00a024\u00a0h,\u00a0\n32\u00a0h,\u00a040\u00a0h,\u00a048\u00a0h,\u00a0and\u00a072\u00a0h.\u00a0\nIn\u00a0 the\u00a0 above\u00a0 experiments,\u00a0 we\u00a0 can\u00a0 conclude\u00a0 that\u00a0 the\u00a0 performance\u00a0 of\u00a0 the\u00a0 STLF\u00a0 In\u2010\nformer\u00a0model\u00a0is\u00a0close\u00a0to\u00a0the\u00a0LSTM\u2010Informer\u00a0model\u00a0proposed\u00a0in\u00a0this\u00a0paper.\u00a0Therefore,\u00a0we\u00a0\nwill\u00a0further\u00a0test\u00a0the\u00a0performance\u00a0of\u00a0the\u00a0two\u00a0models\u00a0in\u00a0STLF.\u00a0In\u00a0the\u00a0evaluation\u00a0model\u00a0per\u2010\nformance\u00a0in\u00a0this\u00a0paper,\u00a0the\u00a0two\u00a0models\u00a0are\u00a0similar\u00a0in\u00a0numerical\u00a0value.\u00a0Therefore,\u00a0we\u00a0fur\u2010\nther\u00a0 compared\u00a0 the\u00a0 differences\u00a0 between\u00a0 the\u00a0 two\u00a0 models\u00a0 from\u00a0 the\u00a0 degree\u00a0 of\u00a0 fitting.\u00a0 As\u00a0\nshown\u00a0in\u00a0Figure\u00a014,\u00a0the\u00a0Informer\u00a0model\u00a0predicts\u00a0the\u00a0power\u00a0load\u00a0consumption\u00a0value\u00a0of\u00a08\u00a0\nh\u00a0with\u00a048\u00a0h\u00a0of\u00a0power\u00a0load\u00a0value\u00a0and\u00a0fits\u00a0the\u00a0predicted\u00a0power\u00a0load\u00a0value\u00a0of\u00a08\u00a0h\u00a0with\u00a0the\u00a0\ntrue\u00a0value.\u00a0As\u00a0shown\u00a0in\u00a0Figure\u00a015,\u00a0it\u00a0shows\u00a0that\u00a0the\u00a0",
            "\u00a0event\u00a0correlation,\u00a0the\u00a0Informer\u00a0model\u00a0successfully\u00a0\nsolves\u00a0the\u00a0long\u2010term\u00a0dependence\u00a0problem.\u00a0Therefore,\u00a0in\u00a0long\u2010term\u00a0power\u00a0load\u00a0forecast\u2010\ning,\u00a0the\u00a0LSTM\u2010Informer\u00a0model\u00a0can\u00a0still\u00a0achieve\u00a0the\u00a0greatest\u00a0results.\u00a0\nFigure 11. Comparison results of six long-term power load models via MSE and MAE metric.\nCombining the results of the two tables, we can conclude that the LSTM-Informer\nmodel has improved the performance of the base learner on the short-term dependence\nproblem. It has relatively good performance compared with other more advanced models\nwith similar architectures. On the issue of long-term dependence, in most cases, it is\nsuperior to other single models. Although the Transformer model performs well in long-\nterm prediction, its accuracy in short-term prediction is less than 50% of the LSTM-Informer\nperformance. If a model is needed for power load forecasting, the LSTM-Informer model\nhas the best performance. It is optimal in both STLF and LTLF.\n4.6.2. Results Analysis\nIn order to further compare the differences between the LSTM-Informer model, In-\nformer model, Autoformer model, Transformer model, Reformer model, and LSTM model,\nwe will analyze the results in detail.",
            "\nis relatively stable. As the time to be predicted increases, the performance of the model\nin STLF decreases slightly. In fact, the LSTM-Informer model performs better than other\nmodels in predicting short-term dependent power loads. On the issue of STLF, the Informer\nmodel is the closest to the model performance proposed in this paper.\nIn order to analyze these methods intuitively, we drew six models of STLF by ranking.\nThey were used to better observe the effect of each model on short-term power load. The\nMSE and MAE results of the six STLF models are shown in Figure 10. Through the ranking\nof MSE index, we can see that LSTM-Informer model and Informer model have the best\nperformance in STLF. The LSTM model and the Transformer model rank relatively stable\nin STLF. Through the ranking of MAE indicators, we found that the Informer model and\nthe LSTM-Informer model have similar performance in STLF. Through the ranking of MAE\nindicators, we found that the Informer model and the LSTM-Informer model have similar\nperformance in STLF. Through two graphs, we found that the time complexity of the model\noptimized based on the Transformer model framework is reduced. However, half of the\nshort-term power load forecasting problem was due to the basic model, and the other half\nof the effect ranking lagged behind the Transformer model. Since then, we have analyzed\nthe model and the Informer model in detail on the issue of STLF.\nElectronics\u00a02023,\u00a012,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 13\u00a0 of\u00a0 19\u00a0\n\u00a0\n\u00a0\n\u00a0\nFigure\u00a010.\u00a0Comparison\u00a0results\u00a0of\u00a0six\u00a0short\u2010term\u00a0power\u00a0load\u00a0models\u00a0via\u00a0MSE\u00a0and\u00a0MAE\u00a0metric.\u00a0\nIn\u00a0the\u00a0second\u00a0comparison\u00a0strategy,\u00a0we\u00a0used\u00a048\u00a0pieces\u00a0of\u00a0data\u00a0(8\u00a0h)\u00a0to\u00a0respectively\u00a0\npredict\u00a0the\u00a0power\u00a0load\u00a0in\u00a0Region3\u00a0of\u00a0144\u00a0pieces\u00a0of\u00a0data\u00a0(24\u00a0h),\u00a0192\u00a0pieces\u00a0of\u00a0data\u00a0(32\u00a0h),\u00a0\n240\u00a0pieces\u00a0of\u00a0data\u00a0(40"
        ],
        "retrieval_context": null,
        "additional_metadata": {
            "evolutions": [
                "In-Breadth"
            ],
            "synthetic_input_quality": 0.5
        },
        "comments": null,
        "tools_called": null,
        "expected_tools": null,
        "source_file": "D:\\ML\\my_rag\\electronics-12-02175.pdf",
        "name": null,
        "custom_column_key_values": null,
        "images_mapping": null
    },
    {
        "input": "Analyze the hybrid LSTM-Informer architecture's dual mechanism for short-term event correlation and long-term dependence in power load forecasting.",
        "actual_output": null,
        "expected_output": "The hybrid LSTM-Informer architecture leverages LSTM's hidden states for short-term event correlation (capturing immediate temporal dependencies) and the Informer's self-attention mechanism for long-term dependence (modeling extended temporal patterns). In short-term power load forecasting (STLF), the LSTM component outperforms Transformer-based models by preserving critical past information, while the Informer's attention mechanism addresses long-term dependencies without gradient vanishing. For long-term predictions, the LSTM-Informer maintains stability (unlike standalone LSTM models) by combining LSTM's short-term focus with the Informer's ability to handle extended sequences, as evidenced by consistent MAE performance across varying prediction horizons (Figures 12\u201313). This dual mechanism ensures superior accuracy in both STLF and long-term load forecasting (LTLF).",
        "context": [
            "Electronics 2023, 12, 2175 14 of 19\nThe experimental results of MAE of our selected model are shown inFigures 12 and 13.\nFigure 12 shows the MAE index of the selected model using 48 h of data to predict the\npower load of 4 h, 8 h, 12 h, 16 h, and 20 h, respectively. We can see that in the issues of STLF,\nthe LSTM model is superior to most of the Transformer structure models, indicating that\npreserving past information through hidden states is more important for STLF. The LSTM-\nInformer model uses the LSTM model at the bottom to better \ufb01nd the time correlation\nbetween data, so that it pays attention to the time correlation of short-term data when\ntraining in the Informer model. Therefore, in short-term power load forecasting, the\nLSTM-Informer model has achieved good results. Figure 13 shows the MAE index of the\nselected model using 8 h of data to predict the power load of 24 h, 32 h, 40 h, 48 h and\n72 h respectively. In the comparison models, the Informer model, Autoformer model, and\nReformer model are all improved models based on the Transformer model. The transformer\nmodel is mainly composed of an encoder and decoder. Its internal self-attention mechanism\nenables data at each time point to pay attention to the data at other time points. In solving\nthe time series problem, it embeds the time-tamp into the data through position encoding.\nThe self-attention mechanism and position coding make it independent of the past hidden\nstate to capture the dependence on the previous data, so it does not produce the gradient\nvanishing problem. At this time, the improved model based on Transformer has achieved\ngood results, and as the time of power load to be predicted increases, the performance is\nalso in a good state. However, as the time of the power load to be predicted increases, the\naccuracy of the LSTM model gradually decreases, indicating that the LSTM model has a\ngradient vanishing problem. However, the accuracy of the LSTM-Informer model proposed\ndoes not show a signi\ufb01cant trend that the performance gradually deteriorates with the\nincrease of the prediction length, indicating that when the LST",
            "\u00a0LTLF.\u00a0\n4.6.2.\u00a0Results\u00a0Analysis\u00a0\nIn\u00a0order\u00a0to\u00a0further\u00a0compare\u00a0the\u00a0differences\u00a0between\u00a0the\u00a0LSTM\u2010Informer\u00a0model,\u00a0In\u2010\nformer\u00a0 model,\u00a0 Autoformer\u00a0 model,\u00a0 Transformer\u00a0 model,\u00a0 Reformer\u00a0 model,\u00a0 and\u00a0 LSTM\u00a0\nmodel,\u00a0we\u00a0will\u00a0analyze\u00a0the\u00a0results\u00a0in\u00a0detail.\u00a0\nThe\u00a0experimental\u00a0results\u00a0of\u00a0MAE\u00a0of\u00a0our\u00a0selected\u00a0model\u00a0are\u00a0shown\u00a0in\u00a0Figures\u00a012\u00a0and\u00a0\n13.\u00a0Figure\u00a012\u00a0shows\u00a0the\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0selected\u00a0model\u00a0using\u00a048\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0\npower\u00a0load\u00a0of\u00a04\u00a0h,\u00a08\u00a0h,\u00a012\u00a0h,\u00a016\u00a0h,\u00a0and\u00a020\u00a0h,\u00a0respectively.\u00a0We\u00a0can\u00a0see\u00a0that\u00a0in\u00a0the\u00a0issues\u00a0of\u00a0\nSTLF,\u00a0the\u00a0LSTM\u00a0model\u00a0is\u00a0superior\u00a0to\u00a0most\u00a0of\u00a0the\u00a0Transformer\u00a0structure\u00a0models,\u00a0indicating\u00a0\nthat\u00a0preserving\u00a0past\u00a0information\u00a0through\u00a0hidden\u00a0states\u00a0is\u00a0more\u00a0important\u00a0for\u00a0STLF.\u00a0The\u00a0\nLSTM\u2010Informer\u00a0model\u00a0uses\u00a0the\u00a0LSTM\u00a0model\u00a0at\u00a0the\u00a0bottom\u00a0to\u00a0better\u00a0find\u00a0the\u00a0time\u00a0correla\u2010\ntion\u00a0between\u00a0data,\u00a0so\u00a0that\u00a0it\u00a0pays\u00a0attention\u00a0to\u00a0the\u00a0time\u00a0correlation\u00a0of\u00a0short\u2010term\u00a0data\u00a0when\u00a0\ntraining\u00a0 in\u00a0 the\u00a0 Informer\u00a0 model.\u00a0 Therefore,\u00a0 in\u00a0 short\u2010term\u00a0 power\u00a0 load\u00a0 forecasting,\u00a0 the\u00a0\nLSTM\u2010Informer\u00a0model\u00a0has\u00a0achieved\u00a0good\u00a0results.\u00a0Figure\u00a013\u00a0shows\u00a0the\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0\nselected\u00a0model\u00a0using\u00a08\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0power\u00a0load\u00a0of\u00a024\u00a0h,\u00a032\u00a0h,\u00a040\u00a0h,\u00a048\u00a0h\u00a0and\u00a072\u00a0",
            "M model is capturing short-\nterm event correlation, the Informer model successfully solves the long-term dependence\nproblem. Therefore, in long-term power load forecasting, the LSTM-Informer model can\nstill achieve the greatest results.\nElectronics\u00a02023,\u00a012,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 15\u00a0 of\u00a0 19\u00a0\n\u00a0\n\u00a0\n\u00a0\nFigure\u00a012.\u00a0The\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0selected\u00a0model\u00a0using\u00a048\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0power\u00a0load\u00a0of\u00a04\u00a0h,\u00a0\n8\u00a0h,\u00a012\u00a0h,\u00a016\u00a0h,\u00a0and\u00a020\u00a0h.\u00a0\n\u00a0\nFigure\u00a013.\u00a0The\u00a0MAE\u00a0index\u00a0of\u00a0the\u00a0selected\u00a0model\u00a0uses\u00a08\u00a0h\u00a0of\u00a0data\u00a0to\u00a0predict\u00a0the\u00a0power\u00a0load\u00a0of\u00a024\u00a0h,\u00a0\n32\u00a0h,\u00a040\u00a0h,\u00a048\u00a0h,\u00a0and\u00a072\u00a0h.\u00a0\nIn\u00a0 the\u00a0 above\u00a0 experiments,\u00a0 we\u00a0 can\u00a0 conclude\u00a0 that\u00a0 the\u00a0 performance\u00a0 of\u00a0 the\u00a0 STLF\u00a0 In\u2010\nformer\u00a0model\u00a0is\u00a0close\u00a0to\u00a0the\u00a0LSTM\u2010Informer\u00a0model\u00a0proposed\u00a0in\u00a0this\u00a0paper.\u00a0Therefore,\u00a0we\u00a0\nwill\u00a0further\u00a0test\u00a0the\u00a0performance\u00a0of\u00a0the\u00a0two\u00a0models\u00a0in\u00a0STLF.\u00a0In\u00a0the\u00a0evaluation\u00a0model\u00a0per\u2010\nformance\u00a0in\u00a0this\u00a0paper,\u00a0the\u00a0two\u00a0models\u00a0are\u00a0similar\u00a0in\u00a0numerical\u00a0value.\u00a0Therefore,\u00a0we\u00a0fur\u2010\nther\u00a0 compared\u00a0 the\u00a0 differences\u00a0 between\u00a0 the\u00a0 two\u00a0 models\u00a0 from\u00a0 the\u00a0 degree\u00a0 of\u00a0 fitting.\u00a0 As\u00a0\nshown\u00a0in\u00a0Figure\u00a014,\u00a0the\u00a0Informer\u00a0model\u00a0predicts\u00a0the\u00a0power\u00a0load\u00a0consumption\u00a0value\u00a0of\u00a08\u00a0\nh\u00a0with\u00a048\u00a0h\u00a0of\u00a0power\u00a0load\u00a0value\u00a0and\u00a0fits\u00a0the\u00a0predicted\u00a0power\u00a0load\u00a0value\u00a0of\u00a08\u00a0h\u00a0with\u00a0the\u00a0\ntrue\u00a0value.\u00a0As\u00a0shown\u00a0in\u00a0Figure\u00a015,\u00a0it\u00a0shows\u00a0that\u00a0the\u00a0",
            "\u00a0event\u00a0correlation,\u00a0the\u00a0Informer\u00a0model\u00a0successfully\u00a0\nsolves\u00a0the\u00a0long\u2010term\u00a0dependence\u00a0problem.\u00a0Therefore,\u00a0in\u00a0long\u2010term\u00a0power\u00a0load\u00a0forecast\u2010\ning,\u00a0the\u00a0LSTM\u2010Informer\u00a0model\u00a0can\u00a0still\u00a0achieve\u00a0the\u00a0greatest\u00a0results.\u00a0\nFigure 11. Comparison results of six long-term power load models via MSE and MAE metric.\nCombining the results of the two tables, we can conclude that the LSTM-Informer\nmodel has improved the performance of the base learner on the short-term dependence\nproblem. It has relatively good performance compared with other more advanced models\nwith similar architectures. On the issue of long-term dependence, in most cases, it is\nsuperior to other single models. Although the Transformer model performs well in long-\nterm prediction, its accuracy in short-term prediction is less than 50% of the LSTM-Informer\nperformance. If a model is needed for power load forecasting, the LSTM-Informer model\nhas the best performance. It is optimal in both STLF and LTLF.\n4.6.2. Results Analysis\nIn order to further compare the differences between the LSTM-Informer model, In-\nformer model, Autoformer model, Transformer model, Reformer model, and LSTM model,\nwe will analyze the results in detail.",
            "\nis relatively stable. As the time to be predicted increases, the performance of the model\nin STLF decreases slightly. In fact, the LSTM-Informer model performs better than other\nmodels in predicting short-term dependent power loads. On the issue of STLF, the Informer\nmodel is the closest to the model performance proposed in this paper.\nIn order to analyze these methods intuitively, we drew six models of STLF by ranking.\nThey were used to better observe the effect of each model on short-term power load. The\nMSE and MAE results of the six STLF models are shown in Figure 10. Through the ranking\nof MSE index, we can see that LSTM-Informer model and Informer model have the best\nperformance in STLF. The LSTM model and the Transformer model rank relatively stable\nin STLF. Through the ranking of MAE indicators, we found that the Informer model and\nthe LSTM-Informer model have similar performance in STLF. Through the ranking of MAE\nindicators, we found that the Informer model and the LSTM-Informer model have similar\nperformance in STLF. Through two graphs, we found that the time complexity of the model\noptimized based on the Transformer model framework is reduced. However, half of the\nshort-term power load forecasting problem was due to the basic model, and the other half\nof the effect ranking lagged behind the Transformer model. Since then, we have analyzed\nthe model and the Informer model in detail on the issue of STLF.\nElectronics\u00a02023,\u00a012,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 13\u00a0 of\u00a0 19\u00a0\n\u00a0\n\u00a0\n\u00a0\nFigure\u00a010.\u00a0Comparison\u00a0results\u00a0of\u00a0six\u00a0short\u2010term\u00a0power\u00a0load\u00a0models\u00a0via\u00a0MSE\u00a0and\u00a0MAE\u00a0metric.\u00a0\nIn\u00a0the\u00a0second\u00a0comparison\u00a0strategy,\u00a0we\u00a0used\u00a048\u00a0pieces\u00a0of\u00a0data\u00a0(8\u00a0h)\u00a0to\u00a0respectively\u00a0\npredict\u00a0the\u00a0power\u00a0load\u00a0in\u00a0Region3\u00a0of\u00a0144\u00a0pieces\u00a0of\u00a0data\u00a0(24\u00a0h),\u00a0192\u00a0pieces\u00a0of\u00a0data\u00a0(32\u00a0h),\u00a0\n240\u00a0pieces\u00a0of\u00a0data\u00a0(40"
        ],
        "retrieval_context": null,
        "additional_metadata": {
            "evolutions": [
                "In-Breadth"
            ],
            "synthetic_input_quality": 1.0
        },
        "comments": null,
        "tools_called": null,
        "expected_tools": null,
        "source_file": "D:\\ML\\my_rag\\electronics-12-02175.pdf",
        "name": null,
        "custom_column_key_values": null,
        "images_mapping": null
    },
    {
        "input": "How do encoder-decoder structure, position encoding, and multi-head attention in the Informer model address long-term dependencies?",
        "actual_output": null,
        "expected_output": "The Informer model addresses long-term dependencies through its encoder-decoder structure, position encoding, and multi-head attention. The encoder captures temporal features, while the decoder integrates encoder outputs and original inputs, setting future prediction values to 0 to focus on relevant historical data. Position encoding (timestamp-based) ensures temporal relationships are preserved across time steps. The multi-head attention mechanism prioritizes data with clear degradation trends, enhancing the model's ability to capture long-term correlations by emphasizing critical dependencies while suppressing noise.",
        "context": [
            "Electronics 2023, 12, 2175 5 of 19\n3.2. Informer\nThe Informer structure is shown in Figure 2. It consists of an encoder and a decoder.\nWhen the Informer model inputs the vector, it will add position encoding (timestamp)\ninformation to mark the position relationship at different times to deal with the correlation\nof time. Compared with the Transformer model, the multi-head attention mechanism of\nthe Informer model focuses more on data with more obvious degradation trends so as to\nbetter solve the long-term dependence problem. The input of the decoder consists of two\nparts\u2014one from the hidden intermediate data features of the output of the encoder and\nthe other from the original input vector. The value to be predicted is assigned to 0, so as to\nprevent the previous position and pay attention to the information of the position to be\npredicted in the future. Following that, the data were connected to the multi-head attention\nmechanism. Finally, the full connection was performed to obtain the \ufb01nal output.\nElectronics\u00a02023,\u00a012,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 5\u00a0 of\u00a0 19\u00a0\n\u00a0\n\u00a0\n3.2.\u00a0Informer\u00a0\nThe\u00a0Informer\u00a0structure\u00a0is\u00a0shown\u00a0in\u00a0Figure\u00a02.\u00a0It\u00a0consists\u00a0of\u00a0an\u00a0encoder\u00a0and\u00a0a\u00a0decoder.\u00a0\nWhen\u00a0the\u00a0Informer\u00a0model\u00a0inputs\u00a0the\u00a0vector,\u00a0it\u00a0will\u00a0add\u00a0position\u00a0encoding\u00a0(timestamp)\u00a0\ninformation\u00a0to\u00a0mark\u00a0the\u00a0position\u00a0relationship\u00a0at\u00a0different\u00a0times\u00a0to\u00a0deal\u00a0with\u00a0the\u00a0correlation\u00a0\nof\u00a0time.\u00a0Compared\u00a0with\u00a0the\u00a0Transformer\u00a0model,\u00a0the\u00a0multi\u2010head\u00a0attention\u00a0mechanism\u00a0of\u00a0\nthe\u00a0Informer\u00a0model\u00a0focuses\u00a0more\u00a0on\u00a0data\u00a0with\u00a0more\u00a0obvious\u00a0degradation\u00a0trends\u00a0so\u00a0as\u00a0to\u00a0\nbetter\u00a0solve\u00a0the\u00a0long\u2010term\u00a0dependence\u00a0problem.\u00a0The\u00a0input\u00a0of\u00a0the\u00a0decoder\u00a0consists\u00a0of\u00a0two\u00a0\nparts\u2014one\u00a0from\u00a0the\u00a0hidden\u00a0intermediate\u00a0data\u00a0features\u00a0of\u00a0the",
            "\nh\u00a0respectively.\u00a0In\u00a0the\u00a0comparison\u00a0models,\u00a0the\u00a0Informer\u00a0model,\u00a0Autoformer\u00a0model,\u00a0and\u00a0\nReformer\u00a0 model\u00a0 are\u00a0 all\u00a0 improved\u00a0 models\u00a0 based\u00a0 on\u00a0 the\u00a0 Transformer\u00a0 model.\u00a0 The\u00a0 trans\u2010\nformer\u00a0model\u00a0is\u00a0mainly\u00a0composed\u00a0of\u00a0an\u00a0encoder\u00a0and\u00a0decoder.\u00a0Its\u00a0internal\u00a0self\u2010attention\u00a0\nmechanism\u00a0 enables\u00a0 data\u00a0 at\u00a0 each\u00a0 time\u00a0 point\u00a0 to\u00a0 pay\u00a0 attention\u00a0 to\u00a0 the\u00a0 data\u00a0 at\u00a0 other\u00a0 time\u00a0\npoints.\u00a0In\u00a0solving\u00a0the\u00a0time\u00a0series\u00a0problem,\u00a0it\u00a0embeds\u00a0the\u00a0time\u2010tamp\u00a0into\u00a0the\u00a0data\u00a0through\u00a0\nposition\u00a0encoding.\u00a0The\u00a0self\u2010attention\u00a0mechanism\u00a0and\u00a0position\u00a0coding\u00a0make\u00a0it\u00a0independ\u2010\nent\u00a0of\u00a0the\u00a0past\u00a0hidden\u00a0state\u00a0to\u00a0capture\u00a0the\u00a0dependence\u00a0on\u00a0the\u00a0previous\u00a0data,\u00a0so\u00a0it\u00a0does\u00a0not\u00a0\nproduce\u00a0 the\u00a0 gradient\u00a0 vanishing\u00a0 problem.\u00a0 At\u00a0 this\u00a0 time,\u00a0 the\u00a0 improved\u00a0 model\u00a0 based\u00a0 on\u00a0\nTransformer\u00a0 has\u00a0 achieved\u00a0 good\u00a0 results,\u00a0 and\u00a0 as\u00a0 the\u00a0 time\u00a0 of\u00a0 power\u00a0 load\u00a0 to\u00a0 be\u00a0 predicted\u00a0\nincreases,\u00a0the\u00a0performance\u00a0is\u00a0also\u00a0in\u00a0a\u00a0good\u00a0state.\u00a0However,\u00a0as\u00a0the\u00a0time\u00a0of\u00a0the\u00a0power\u00a0load\u00a0\nto\u00a0be\u00a0predicted\u00a0increases,\u00a0the\u00a0accuracy\u00a0of\u00a0the\u00a0LSTM\u00a0model\u00a0gradually\u00a0decreases,\u00a0indicating\u00a0\nthat\u00a0 the\u00a0 LSTM\u00a0 model\u00a0 has\u00a0 a\u00a0 gradient\u00a0 vanishing\u00a0 problem.\u00a0 However,\u00a0 the\u00a0 accuracy\u00a0 of\u00a0 the\u00a0\nLSTM\u2010Informer\u00a0model\u00a0proposed\u00a0does\u00a0not\u00a0show\u00a0a\u00a0significant\u00a0trend\u00a0that\u00a0the\u00a0performance\u00a0\ngradually\u00a0deteriorates\u00a0with\u00a0the\u00a0increase\u00a0of\u00a0the\u00a0prediction\u00a0length,\u00a0indicating\u00a0that\u00a0when\u00a0the\u00a0\nLSTM\u00a0model\u00a0is\u00a0capturing\u00a0short\u2010term",
            "\u00a0output\u00a0of\u00a0the\u00a0encoder\u00a0and\u00a0\nthe\u00a0other\u00a0from\u00a0the\u00a0original\u00a0input\u00a0vector.\u00a0The\u00a0value\u00a0to\u00a0be\u00a0predicted\u00a0is\u00a0assigned\u00a0to\u00a00,\u00a0so\u00a0as\u00a0\nto\u00a0prevent\u00a0the\u00a0previous\u00a0position\u00a0and\u00a0pay\u00a0attention\u00a0to\u00a0the\u00a0information\u00a0of\u00a0the\u00a0position\u00a0to\u00a0be\u00a0\npredicted\u00a0in\u00a0the\u00a0future.\u00a0Following\u00a0that,\u00a0the\u00a0data\u00a0were\u00a0connected\u00a0to\u00a0the\u00a0multi\u2010head\u00a0atten\u2010\ntion\u00a0mechanism.\u00a0Finally,\u00a0the\u00a0full\u00a0connection\u00a0was\u00a0performed\u00a0to\u00a0obtain\u00a0the\u00a0final\u00a0output.\u00a0\n\u00a0\nFigure\u00a02.\u00a0Structure\u00a0of\u00a0Informer.\u00a0\nQ ,\u00a0 K ,\u00a0 V vector\u00a0constitutes\u00a0the\u00a0attention\u00a0mechanism.\u00a0Suppose\u00a0 iq ,\u00a0 ik ,\u00a0 iv \u00a0 is\u00a0the\u00a0 i th\u00a0\nrow\u00a0of\u00a0matrix\u00a0 QKV.\u00a0The\u00a0line\u00a0 i \u00a0 of\u00a0the\u00a0final\u00a0output\u00a0 z \u00a0 can\u00a0be\u00a0expressed\u00a0as:\u00a0\n(| )\n(,)(, ,) [] (, )\nji\nij\nij p k q j\nj ill\nkq kAq KV v E v kq k\uf03d\uf03d\uf0e5 \uf0e5\n\u00a0 (2)\nThe\u00a0evaluation\u00a0of\u00a0the\u00a0 i th\u00a0query\u00a0sparsity\u00a0is:\u00a0\n11\n1(, )\nT\nijk K\nqk TL L\nijd\ni\njj K\nqkMq K I n e L d\uf03d\uf03d\n\uf03d\uf02d \uf0e5\uf0e5 \u00a0 (3)\nThe\u00a0final\u00a0attention\u00a0mechanism\u00a0is:\u00a0\n(, ,) m a x ( )\nTQKA Q K V Soft V\nd\n\uf03d \u00a0 (4)\nThe\u00a0 Informer\u00a0 model\u00a0 encodes\u00a0 the\u00a0 data\u00a0 and\u00a0 adds\u00a0 position\u00a0 coding\u00a0 through\u00a0 the\u00a0\ntimestamp\u00a0 so\u00a0 that\u00a0the\u00a0data\u00a0 have\u00a0 time\u00a0series\u00a0 dependence.\u00a0 EncoderStack\u00a0 is\u00a0 composed\u00a0 of\u00a0\nmultiple\u00a0Encoder",
            "\u00a0layers\u00a0and\u00a0distilling\u00a0layers.\u00a0The\u00a0EncoderStack\u00a0structure\u00a0is\u00a0shown\u00a0in\u00a0Fig\u2010\nure\u00a03.\u00a0The\u00a0Layer\u00a0Normalization\u00a0[34]\u00a0formula\u00a0is\u00a0expressed\u00a0as:\u00a0\nFigure 2. Structure of Informer.\nQ, K, V vector constitutes the attention mechanism. Suppose qi, ki, vi is the ith row of\nmatrix QKV. The line i of the \ufb01nal output z can be expressed as:\nA(qi, K, V) = \u2211\nj\nk(qi, kj)\n\u2211l k(qi, kl) vj = Ep(kj|qi)[vj] (2)\nThe evaluation of the ith query sparsity is:\nM(qi, K) = In\nLk\n\u2211\nj=1\ne\nqi kT\nj\u221a\nd \u2212 1\nLK\nLK\n\u2211\nj=1\nqikT\nj\u221a\nd\n(3)\nThe \ufb01nal attention mechanism is:\nA(Q, K, V) = So f tmax( QKT\n\u221a\nd\n)V (4)\nThe Informer model encodes the data and adds position coding through the timestamp\nso that the data have time series dependence. EncoderStack is composed of multiple",
            "Electronics 2023, 12, 2175 11 of 19\nElectronics\u00a02023,\u00a012,\u00a0x\u00a0FOR\u00a0PEER\u00a0REVIEW\u00a0 11\u00a0 of\u00a0 19\u00a0\n\u00a0\n\u00a0\nlength\u00a0of\u00a0the\u00a0required\u00a0historical\u00a0data\u00a0is\u00a0h.\u00a0The\u00a0length\u00a0of\u00a0the\u00a0power\u00a0load\u00a0to\u00a0be\u00a0predicted\u00a0is\u00a0\nw.\u00a0\n\u00a0\nFigure\u00a09.\u00a0Data\u00a0set\u00a0preprocessing.\u00a0\n4.4.\u00a0Methods\u00a0for\u00a0Comparison\u00a0\nThis\u00a0paper\u00a0selects\u00a0the\u00a0more\u00a0advanced\u00a0models\u00a0for\u00a0comparison.\u00a0\nInformer\u00a0[47]:\u00a0The\u00a0model\u00a0improves\u00a0on\u00a0the\u00a0basic\u00a0Transformer\u00a0model\u00a0and\u00a0is\u00a0more\u00a0suit\u2010\nable\u00a0for\u00a0long\u2010term\u00a0prediction\u00a0problems.\u00a0\nTransformer\u00a0[18]:\u00a0At\u00a0the\u00a0beginning,\u00a0it\u00a0was\u00a0a\u00a0very\u00a0famous\u00a0text\u00a0processing\u00a0model.\u00a0It\u00a0\nfirst\u00a0introduced\u00a0the\u00a0attention\u00a0mechanism\u00a0and\u00a0was\u00a0later\u00a0applied\u00a0to\u00a0various\u00a0fields,\u00a0thus\u00a0pro\u2010\nmoting\u00a0the\u00a0development\u00a0of\u00a0various\u00a0fields.\u00a0For\u00a0the\u00a0time\u00a0series\u00a0problem,\u00a0the\u00a0Transformer\u00a0\nmodel\u00a0shows\u00a0superior\u00a0performance\u00a0in\u00a0capturing\u00a0the\u00a0relationship\u00a0of\u00a0the\u00a0gas\u00a0dependence\u00a0\nproblem.\u00a0The\u00a0transformer\u00a0model\u00a0shows\u00a0great\u00a0potential\u00a0in\u00a0solving\u00a0the\u00a0LSTF\u00a0problem\u00a0[35].\u00a0\nAutoformer\u00a0[19]:\u00a0The\u00a0model\u00a0based\u00a0on\u00a0Transformer\u00a0introduces\u00a0seasonal\u00a0and\u00a0periodic\u00a0\nterms\u00a0to\u00a0better\u00a0solve\u00a0the\u00a0long\u2010term\u00a0prediction\u00a0problem.\u00a0\nReformer\u00a0[48]:\u00a0The\u00a0use\u00a0of\u00a0locality\u2010sensitive\u00a0hashing\u00a0instead\u00a0of\u00a0the\u00a0original\u00a0dot\u00a0prod\u2010\nuct\u00a0Attention\u00a0reduces\u00a0the\u00a0time\u00a0complexity\u00a0of\u00a0Transformer,\u00a0but\u00a0the\u00a0performance\u00a0is\u00a0compa\u2010\nrable.\u00a0It\u00a0has\u00a0higher\u00a0memory\u00a0efficiency\u00a0and\u00a0speed\u00a0in\u00a0long\u00a0sequences."
        ],
        "retrieval_context": null,
        "additional_metadata": {
            "evolutions": [
                "Reasoning"
            ],
            "synthetic_input_quality": 1.0
        },
        "comments": null,
        "tools_called": null,
        "expected_tools": null,
        "source_file": "D:\\ML\\my_rag\\electronics-12-02175.pdf",
        "name": null,
        "custom_column_key_values": null,
        "images_mapping": null
    }
]